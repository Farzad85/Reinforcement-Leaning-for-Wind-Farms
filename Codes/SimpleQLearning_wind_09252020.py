# -*- coding: utf-8 -*-
"""RL-Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w5y7osU2pJOszB4kpJ18j6s0vqYdhiJO

# Q-Learning for a Wind Turbine Yaw Alignment to Maximize Power Production

This is a sample program for executing the Q-Learning algorithm to find the best nacelle angle in terms of power production.

*   The wind direction and nacelle angle are states of the system and change in the nacelle angle is the action. The states and actions are discretized to adapt to q-Learning algorithm requirements.

*   A simple model for the power production (reward) is adopted which returns the cosine of the difference between the actual wind direction and the nacelle angle. A fully random method is used for generating actions at each training step. 

*   The Q matrix is filled by the Q of the current state and action, estimation of the best Q for the next step, and the reward value. 

*   In the training phase, the Q matrix is filled in a loop through random sampling of the states and action.

*   For the test, the process starts with a desired initial state, and then the actions are chosen based on the corresponding largest Q value.
"""

import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import matplotlib
from tqdm import tqdm 
import floris.tools as wfct

#%% Floris

# initialize FLORIS interface
fi = wfct.floris_interface.FlorisInterface("input.json")
fi.reinitialize_flow_field(layout_array=[[0], [0]])      # system parameter

# define farm power generator
def rewardFunc(states, actions):

    # changing the wind farm configuration
    wind_direction = states[0]
    nacelle_angle = states[1]
    yaw_angle = wind_direction - nacelle_angle
    
    fi.reinitialize_flow_field(wind_direction = wind_direction)

    # calculating the power
    fi.calculate_wake(yaw_angles = yaw_angle)
    power_0 = fi.get_farm_power()
    fi.calculate_wake(yaw_angles = yaw_angle - actions)
    power_1 = fi.get_farm_power()

    # computing the reward
    reward = power_1 - power_0

    
    # plot and visualization
    hor_plane = fi.get_hor_plane()
    fig, ax = plt.subplots()
    wfct.visualization.visualize_cut_plane(hor_plane, ax=ax)
    plt.show()
    # new_state = [states[0], states[1] + action]
    
    return reward

direction = -10
nacelle_angle = 0
action = 0
reward = rewardFunc([direction, nacelle_angle], 0)

#%%
# States Definition
wind_dir = np.linspace(-30,30,13)
Nacelle_ang = np.linspace(-30,30,13)

# Action Definition
actions = np.array([-5,0,+5])

# R matrix
A = np.array(list(itertools.product(wind_dir, Nacelle_ang,actions)))
table = pd.DataFrame({'wind_dir': A[:,0],'Nacelle_ang': A[:,1], 'action': A[:,2]})

#Removing Infeasible Action-State Rows
table.drop(table[(table['wind_dir'] - table['Nacelle_ang'] == -60) & (table['action'] == 5)].index, inplace = True)
table.drop(table[(table['wind_dir'] - table['Nacelle_ang'] ==  60) & (table['action'] == -5)].index, inplace = True)

# Reward Function Generation
# def rewardFunc(row):
#     return np.cos((row['wind_dir'] - (row['Nacelle_ang'] + row['action']))*np.pi/180)

# Function to Move to the Next State by the Input Action
def statePropagation(state, action):
    return(state[0] , state[1] + action)

# Reward Table    
table['reward'] = table.apply(lambda row: rewardFunc([row['wind_dir'],row['Nacelle_ang']], row['action']), axis = 1)
R_table = pd.pivot_table(table, values='reward', index=['wind_dir' , 'Nacelle_ang'],
                    columns=['action'], aggfunc=np.sum)

# Q matrix
Q = pd.DataFrame(0, index = R_table.index, columns = R_table.columns)

# Gamma and Alpha (learning parameter).
gamma = 0.9
alpha = 0.9

# Function to Identify Possible Actions
def available_actions(state):
    current_state_row = R_table.loc[state]
    av_act = ~np.isnan(current_state_row.values)
    av_act = actions[av_act]
    return av_act.astype(int)

#%%  Training Loop to Fill the Q Matrix
for i in tqdm(range(1000)):
    current_state = np.random.choice(R_table.index,1, replace = False)
    available_act = available_actions(current_state[0])
    action = np.random.choice(available_act)
    next_state = statePropagation(current_state[0] ,action)
    Q_next_max = np.max(Q.loc[current_state].values)
    Q.loc[current_state, action] = Q.loc[current_state, action] + alpha * (R_table.loc[current_state,action] + gamma * Q_next_max - Q.loc[current_state, action])
    
Q.head(10)


#%%  Determining the States and Action Sequence

# Initial State
states = [(20, -10)]
episodes = range(20)
action_sequence = []
for i in episodes:
    next_step_index = np.where(Q.loc[states[-1]].values == np.max(Q.loc[states[-1]].values))[0]
    if next_step_index.shape[0] > 1:
        next_step_index = int(np.random.choice(next_step_index, size = 1))
    else:
        next_step_index = int(next_step_index)
    av_act = actions[next_step_index]
    next_state = statePropagation(states[-1],av_act)
    states.append(next_state)
    action_sequence.append(av_act)

print('State Sequence:        ')
print(states)
print('\nAction Sequence:        ')
print(action_sequence)

font = {'size'   : 16}
matplotlib.rc('font', **font)

fig, ax = plt.subplots(1,2, figsize = (30,8))
ax[0].step(episodes, [s[0] - s[1] for s in states[:-1]])
ax[0].yaxis.set_major_locator(MaxNLocator(integer=True))
ax[0].set_title('Yaw Misalignment vs Steps')
ax[0].set_xlabel('Episode Number')
ax[0].set_ylabel('Yaw Misalignment (deg)')

ax[1].step(episodes, action_sequence)
ax[1].yaxis.set_major_locator(MaxNLocator(integer=True))
ax[1].set_title('Actions vs Steps')
ax[1].set_xlabel('Episode Number')
ax[1].set_ylabel('Action (deg)')

